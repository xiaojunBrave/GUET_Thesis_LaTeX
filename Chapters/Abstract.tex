%
%  =======================================================================
%  ····Y88b···d88P················888b·····d888·d8b·······················
%  ·····Y88b·d88P·················8888b···d8888·Y8P·······················
%  ······Y88o88P··················88888b·d88888···························
%  ·······Y888P··8888b···88888b···888Y88888P888·888·88888b·····d88b·······
%  ········888······"88b·888·"88b·888·Y888P·888·888·888·"88b·d88P"88b·····
%  ········888···d888888·888··888·888··Y8P··888·888·888··888·888··888·····
%  ········888··888··888·888··888·888···"···888·888·888··888·Y88b·888·····
%  ········888··"Y888888·888··888·888·······888·888·888··888··"Y88888·····
%  ·······························································888·····
%  ··························································Y8b·d88P·····
%  ···························································"Y88P"······
%  =======================================================================
% 
%  -----------------------------------------------------------------------
% Author       : 焱铭
% Date         : 2023-07-19 13:15:53 +0800
% LastEditTime : 2023-08-24 19:24:30 +0800
% Github       : https://github.com/YanMing-lxb/
% FilePath     : \GUET_Thesis_LaTeX\Chapters\Abstract.tex
% Description  : 
%  -----------------------------------------------------------------------
%

% !Mode:: "TeX:UTF-8"


\begin{ChineseAbstract}
  随着农业自动化与精准作业需求的持续增长，温室环境下的机器人授粉技术逐渐成为研究热点。然而，受限于花朵姿态不规则、背景干扰复杂以及机械臂操作刚性强等因素，实现高精度的自主授粉仍是一个难题。为此，本文提出一种基于手眼协同的机械臂精准授粉方法，构建了一个自主授粉系统，集成了花朵智能感知、位姿重建、柔顺控制及末端误差补偿等关键模块。主要研究工作包括：
  
  （1）目标感知方面：针对花朵检测难、实例分割精度低的问题，本文对比分析了Mask R-CNN与YOLACT两种深度实例分割网络，在自建数据集上进行训练，并系统评估其在不同光照与遮挡条件下的分割精度、推理速度及鲁棒性。
  
  （2）位姿估计方面：针对目标位姿难以准确获取的问题，本文在完成目标检测与分割的基础上，提出了一种融合对称空间信息的三维位姿重建方法。该方法结合深度图去噪、三维坐标计算与手眼标定技术，实现了对目标花朵空间位置与姿态的高精度估计。
  
  （3）柔顺控制方面：为提升机械臂在授粉过程中的可达性与安全性，设计了一种“粗到精”两阶段的视觉伺服控制策略，并引入三次样条插值算法生成连续、平滑的关节空间轨迹，从而显著提高了操作的柔顺性与稳定性。
  
  （4）误差补偿方面：针对实际操作中存在的末端定位误差，进一步提出一种基于Transformer架构的视觉误差预测网络，能够实时估计授粉末端与目标之间的平移与旋转误差。实验结果表明，该方法在平移误差方面的精度提升了46.47\%，平均单花授粉效率提升了50.9\%。
  
  在面向多目标番茄花的连续授粉实验中，系统达成了86.19\%的授粉成功率，验证了所提出方法在复杂温室农业环境下的高效性与实用性。
   
    

\ChineseKeyword{精准授粉；手眼协同；柔顺控制；误差估计；}
\end{ChineseAbstract}


\begin{EnglishAbstract}
With the growing demand for agricultural automation and precision operations, robotic pollination in greenhouse environments has emerged as a research hotspot. However, achieving high-precision autonomous pollination remains a significant challenge due to factors such as irregular flower postures, complex background interference, and the inherent rigidity of robotic manipulators. To address these issues, this thesis proposes a vision-guided robotic pollination method based on hand–eye coordination and develops an autonomous pollination system that integrates key modules including intelligent floral perception, pose estimation, soft motion control, and end-effector error compensation. The main contributions are as follows:

(1) Floral perception: To tackle the difficulties of flower detection and the low accuracy of instance segmentation in agricultural scenarios, this study compares two state-of-the-art deep instance segmentation networks—Mask R-CNN and YOLACT. Their performance is systematically evaluated under varying lighting and occlusion conditions in terms of segmentation accuracy, inference speed, and robustness.

(2) Pose estimation: To address the difficulty in accurately acquiring flower poses, a 3D pose reconstruction method that incorporates symmetry-aware spatial constraints is proposed. By combining depth map denoising, 3D coordinate computation, and hand–eye calibration, the method achieves high-precision estimation of the spatial position and orientation of target flowers.

(3) Soft control: To improve the reachability and operational safety of the robotic arm during pollination, a two-stage "coarse-to-fine" visual servoing strategy is designed. Additionally, cubic spline interpolation is used to generate continuous and smooth joint-space trajectories, significantly enhancing the compliance and stability of the robotic motion.

(4) Error compensation: To address end-effector positioning errors during operation, a vision-based error prediction network based on the Transformer architecture is introduced. This model enables real-time estimation of translational and rotational deviations between the end-effector and the target. Experimental results show a 46.47\% improvement in translational accuracy and a 50.9\% increase in average single-flower pollination efficiency.

In multi-target tomato flower pollination experiments, the system achieved a pollination success rate of 86.19\%, demonstrating the effectiveness and practicality of the proposed method in complex greenhouse environments.

\EnglishKeyword{precision pollination;hand-eye coordination;soft control;offset error estimation;}
\end{EnglishAbstract}